\chapter{Background}
\label{chapter:background} 

\section{Overview}
This chapter focuses on the fundamental concepts for the rest of the chapters. We begin with introducing hybrid speech recognition systems and then compare them with end to end speech recognition systems in Section \ref{section:asr}. Then we discuss the challenges associated with training an end to end speech recognition model and discuss the different types of deep neural networks used in an ASR application.

\section{Speech Recognition}
\label{section:asr}

\subsection{Hybrid speech recognition systems}
%The terminology is a little unclear even in the field, but Hybrid mostly refers to HMM/DNN - but I guess what you're describing is more generally HMM based ASR. You can use this terminology too, cause nowadays HMM-based ASR is always HMM/DNN ASR; just be aware of the connotations.
\label{section:hybridasr}
Hybrid method of speech recognition use multiple steps which include feature generation, acoustic modelling, language modelling and a variety of other methods which then combine to form one pipeline which provides speech to text results. Figure \ref{fig:hyrid_asr_model} shows this architecture.

\begin{figure}[ht]
  \begin{center}
    % below the size of the figure has been reduced for example
    \includegraphics[width=\textwidth]{images/Hybrid ASR System.pdf} 
    \caption{A hybrid ASR system architecture}
    \label{fig:hyrid_asr_model}
  \end{center}
\end{figure}

In the HMM-based architecture, the first step is preprocessing the speech signal to remove any unwanted noise and then convert to the required format for the next steps. Next, apply feature extraction methods to generate vectors which are used in the decoder. The decoder which consists of multiple models including acoustic models, pronunciation dictionary and language models then process the feature vectors from the previous stage.

\subsection{End to end speech recognition systems}
\label{section:e2easr}
In end to end speech recognition systems, a single model, usually based on deep learning, supersedes the hybrid ASR system stages. A deep learning model combined with an external language model can achieve higher performance than hybrid speech pipelines, while being simple to train. These systems rely on large neural networks and are trained on multiple GPUs and thousands of hours of speech data. Since the system learns the whole task end-to-end, specialized components for capturing finer details of the speaker or other noise filtering components are not essential. On the contrary, previous experiments have shown that end to end models stand out in the cases where robustness is crucial. \cite{Hannun2014DeepRecognition}. 

\emph{In an end to end deep learning ASR system, we can achieve performance gains by improving three main components: model architecture, training data and computational infrastructure. The focus of this thesis is hence to increase the scale of data while making maximum use of the available computational resources, and trying out various experiments with different model architectures to analyse the effect it has on final performance of the system.}

\section{What are Deep Neural Networks?}
A deep neural network is a neural network with many hidden layers. Theoretically, deep neural networks can be trained to model non-linear models and functions for very high dimensional data. Traditionally, it was quite challenging to train deep neural networks, but there was a resurgence in the usage of deep neural networks in the previous decade, also in the field of speech recognition \cite{Dahl2012Context-DependentRecognition, Morgan2012DeepRecognition, DengRECENTMICROSOFT, Hannun2014DeepRecognition}. 

One of the first deep neural network was a Multilayer Perceptron (MLP), which consists of an input layer, an output layer and a hidden layer. Figure \ref{fig:mlp} shows this architecture. 

\begin{figure}[ht]
  \begin{center}
    % below the size of the figure has been reduced for example
    \includegraphics[width=\textwidth]{images/MLP.pdf} 
    \caption{An MLP architecture}
    \label{fig:mlp}
  \end{center}
\end{figure}

Each layer of this network consists of multiple neurons, and each of these neurons can be represented using a vector, and all the neurons in a layer can then be represented using a weights matrix that determines how much a neuron should take part in the decision-making process. At each neuron, multiply the input signal with its corresponding weights and then added to have a weighed sum as the output of a layer. This can be represented as,

\[ z = w_1x_1 + w_2x_2 + .. + w_nx_n \]

In the above formula $z$ is the weighted sum output, $w$ is the weight, $x$ is an input and there are $n$ number of inputs. An activation function like tanh, sigmoid, ReLU, etc is applied to the weighted sum to get the output of the layer. 

The training process is a repetition of multiple such training batches until all the data is traversed once. This is referred to as an epoch. From the output, calculate the error by comparing it with the ground truth. This error is then backpropagated to all the previous layers to improve the network weights. Backpropagation requires an optimization algorithm like Stochastic Gradient Descent (SGD), Adaptive Momentum Estimation (Adam), etc. At the end of the training process, the main goal is to achieve high performance of the overall model by iteratively setting the right values for the weight matrices in each neuron.

\subsection {Recurrent Neural Networks}
A recurrent neural network (RNN) is a type of neural network that can predict a future state by taking the previous states as input. \cite{Graves2013SpeechNetworks} This network can be thought of as having memory because it considers the context of the input provided to determine the output state. \cite{HagnerRecurrentModel}

Figure \ref{fig:rnn} represents a basic unit of an RNN. It consists of an extra synapse that loops back into itself. In practise, this means that each unit receives a new input and also receives the output from the previous units as an input.
\begin{figure}[ht]
  \begin{center}
    % below the size of the figure has been reduced for example
    \includegraphics[width=\textwidth]{images/rnn.png} 
    \caption{A basic recurrent neural network cell and a way to visualize it
unrolling through time, from time step t-3 to t \cite{HagnerRecurrentModel}}
    \label{fig:rnn}
  \end{center}
\end{figure}

Consider an input series $\boldsymbol{x}=\left(x_{1}, \ldots, x_{T}\right)$, an RNN calculates the hidden sequence $\boldsymbol{h}=\left(h_{1}, \ldots, h_{T}\right)$ and the output sequence $\boldsymbol{y}=$ $\left(y_{1}, \ldots, y_{T}\right)$ by looping the following equations from $t=1$ to $T$ :

$$
\begin{aligned}
&h_{t}=\mathcal{H}\left(w_{xh} x_{t}+w_{hh} h_{t-1}+b_{h}\right) \\
&y_{t}=w_{h y} h_{t}+b_{y}
\end{aligned}
$$

where the $w$ terms denote weight matrices, the $b$ terms denote bias vectors and $\mathcal{H}$ is the hidden layer function. \cite{Graves2013SpeechNetworks}

The drawback of an RNN is that they use only the previous output to get the current context, but for speech recognition there is value to also utilize the future states too. A more advanced variation of RNN is hence a Bidirectional RNN (BRNN) \cite{Schuster1997BidirectionalNetworks} processing of the input happens in both directions using two hidden layers and then this used as input to a single output layer. 

\subsubsection{Long-Term Short Memory (LSTM)}
By modifying the RNN architecture, a Long short-term memory (LSTM) cell \cite{Hochreiter1997LongMemory} passes another state vector to the next time step, along with the hidden state. The cell state variable enables the useful long term based information to be preserved because this is modified only when there is new information. To control the cell state, gates which multiply a signal that is 1 or 0 is used. Each gate has a weight matrix which are trained by iterating over the input data. The original work use two gates which are "input" gate, which control what information is essential and hence adds to the current state, and "output" gate which control what information needs to be written to the hidden state. More recently, a third gate called a "forget" gate controls deleting the unwanted information  from the cell state to stop cells growing in a boundless manner. \cite{Gers2000LearningLSTM}.

\begin{figure}[ht]
  \begin{center}
    % below the size of the figure has been reduced for example
    \includegraphics[width=\textwidth]{images/lstm.png} 
    \caption{A basic LSTM cell for a single time step \cite{Enarvi2018ModelingRecognition}}
    \label{fig:lstm}
  \end{center}
\end{figure}

Figure \ref{fig:lstm} shows the architecture of an LSTM layer with all the three gates. The three gates take in identical input, which also depends on how the definition of the network. In most cases, the input to LSTM cell is the combination of the hidden state of the previous time step ($c_{t-1}$) and the output of the previous layer. Each gate contains 2 weight matrices, one each for the two inputs, and a bias vector. (Figure \ref{fig:lstm} shows only a weight matrix per gate.) 


\subsection {Convolutional Neural Networks}
Convolutional neural network (CNN) is a branch of neural networks, which are hierarchical in nature and rely on convolution operations to process input. CNNs specialize in performing tasks like image classification, object detection, etc on images and videos. CNNs perform well when spatial features are important to perform the given task. \cite{KrizhevskyImageNetNetworks}

The architecture of a general CNN is alternating layers of convolution with subsampling layers. \cite{CiresanFlexibleClassification} The convolution layer has the parameters by size, kernel size, skipping factors and inter connections between the layers which are tuned to achieve high performance. Figure \ref{fig:cnn} shows the architecture of AlexNet, one of the most popular CNNs for image classification. \cite{KrizhevskyImageNetNetworks}. The architecture consists of convolutional layers altered with max pooling layers which act as sub sampling layers. The last layer of the network is using a fully connected layer to reduce the final dimensionality to match the required output shape.
\begin{figure}[ht]
  \begin{center}
    % below the size of the figure has been reduced for example
    \includegraphics[width=\textwidth]{images/cnn.png} 
    \caption{AlexNet, a popular CNN architecture  \cite{KrizhevskyImageNetNetworks}}
    \label{fig:cnn}
  \end{center}
\end{figure}

More recently, convolutional neural networks have become popular for end to end speech recognition applications as well. \cite{Zhang2017VeryRecognition} The focus is on using convolutional layers along with a recurrent neural network. Unlike a typical RNN with a fully connected layer, these networks replace it with a convolution layer because it is better at using the input topology to produce the required output. These neural networks which are a combination of convolutional layers and a recurrent network are hence termed as \emph{Convolutional Recurrent Deep Neural Network (CRDNN)}. 

\subsection {Listen, Attend and Spell}
Encoder-decoder type architecture models have become extremely popular over the last few years and one of the variations in this branch of neural networks are the attention-based models. \cite{VaswaniAttentionNeed,  Prabhavalkar2017ARecognition} Listen, Attend and Spell (LAS) models are a combination of two sub-modules: the Listener module and the Attend-Speller module. The listener is an acoustic model encoder which converts the input signal to an embedding representation. The Attend-Speller module acts as a decoder module and takes the embeddings generated to produce a probability distribution for the output character sequences. \cite{Zhang2017VeryRecognition, Chan2016ListenRecognition}

\begin{figure}[ht]
  \begin{center}
    % below the size of the figure has been reduced for example
    \includegraphics[width=0.3\textwidth]{images/las.png} 
    \caption{Architecture Diagram for a LAS model  \cite{Chiu2017State-of-the-artModels}}
    \label{fig:las}
  \end{center}
\end{figure}

Figure \ref{fig:las} represents the architecture of a LAS model with the Encoder as the Listen Module and Attention and Decoder as the Attend-Speller module. The attention mechanism provides the context value. The below equations represents the functions in mathematical form.

$$
\begin{aligned}
\mathbf{h} &=\operatorname{Listen}(\mathbf{x}) \\
P\left(y_{i} \mid \mathbf{x}, y_{<i}\right) &=\text { AttendSpell }\left(y_{<i}, \mathbf{h}\right)
\end{aligned}
$$
where $ \mathbf{h}$ is the embedding representation, the output from the Listen module and $P\left(y_{i} \mid \mathbf{x}, y_{<i}\right)$ represents the probability of the output character based on the input signal and the previous output characters.

Let $\mathbf{x}=\left(x_{1}, \ldots, x_{T}\right)$ be the input sequence and $\mathrm{y}$ be the output sequence of characters. LAS models produce for every character an output $y_{i}$ which is a conditional distribution based on the previously occurring characters $y_{<i}$ and the original signal $\mathrm{x}$ by making use of the chain rule for probabilities:

$$
P(\mathbf{y} \mid \mathbf{x})=\prod_{i} P\left(y_{i} \mid \mathbf{x}, y_{<i}\right)
$$

This defined the end to end nature of the LAS models because it generates a probability distribution for the output characters sequence right from the input signal. 

\subsubsection{Connectionist Temporal Classification}
The connectionist temporal classification criteria is a way of training end-to-end models when the target sequence and the input sequence do not match exactly in length. It eliminates the need to align the target labels on a frame to frame basis with the input training signal. 

From the previous encoder-decoder architecture, the difference is that the conditional probability of the output character at each time step, the encoder output embeddings $\mathbf{h}$, are then fed to a softmax layer which considers the whole set of blank-augmented output symbols to predict a probability distribution output similar in nature to the typical encode decoder output.


\section{SpeechBrain Toolkit}
Speech recognition have always been driven forward due to the critical role played by open-source toolkits such as HTK \cite{Young1995TheBook}, Kaldi \cite{PoveyTheToolkit}, etc. More recently, general purpose deep learning frameworks like TensorFlow \cite{AbadiTensorFlow:Systems} and PyTorch \cite{Paszke2019PyTorch:Library} have become useful for speech recognition tasks and this has led to newer toolkits like ESPNet \cite{Watanabe2018ESPnet:Toolkit}. SpeechBrain \cite{RavanelliSpeechBrain:Toolkit} is a toolkit designed to be flexible and used for multiple tasks to speed up research and development of speech related technologies.  SpeechBrain is a PyTorch-based toolkit and equipped to perform multiple tasks at once like recognize speech, understand its content, emotion, language, and speakers. 

\subsection{\inlinecode{Brain} class}
The SpeechBrain's core functionalities revolve around the \inlinecode{Brain} class, which defines a general training loop. The \inlinecode{Brain.fit()} method trains the models, and the Figure \ref{fig:brain} shows the basic components of the method. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{images/brainclass.png} 
    \caption{\inlinecode{Brain.fit()} method illustration \cite{RavanelliSpeechBrain:Toolkit}}
    \label{fig:brain}
  \end{center}
\end{figure}

Training of most DNN models can be completed with a few lines of code. The Brain class also handles repetitive boilerplate code. It also takes care of validation, learning rate scheduling, recoverable fault tolerant checkpointing etc. Training a model happens through a python script and run from the command line with the combination of a configuration file: 
\begin{verbatim} python train.py train.yaml\end{verbatim}

\subsection{HyperPyYAML}
The configuration file is in YAML format which is human-readable and can be used to define the model architecture, the data files, the parameters of the model, hyperparameters for training and other characteristics of the pipeline. SpeechBrain utilities HyperPyYAML \footnote{speechbrain/HyperPyYAML:  Extensions  to  YAML  syntax  for  better python interaction. \href{https://github.com/speechbrain/HyperPyYAML}{https://github.com/speechbrain/HyperPyYAML} } which makes several extensions to the default YAML file format. The most important addition out of them is the easier object creation. An example:

\begin{verbatim}
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: 100
\end{verbatim}

This tag with prefix \inlinecode{!new:} creates an instance of the specified class with an option to pass keyword arguments by using a mapping node like in the example above.

The next extension is the use of prefix, \inlinecode{!name} which simplifies the creation of an interface for specifying a function or class or other static Python entity. Below is an example using that.
\begin{verbatim}
opt_class: !name:torch.optim.Adam
    lr: !ref <lr>
\end{verbatim}
The prefix \inlinecode{!ref} is also an extension to the alias system used in YAML files. It takes keys in angles brackets and searched for the key within that YAML file. It can be used also for string interpolation, concatenation etc. It also has the \inlinecode{!include} prefix to import other YAML files to be used to reference in the current YAML file. This can be used to make the configuration files modular as well. 

\begin{verbatim}
dataset_parameters: !include:dataset.yaml
tokenizer_parameters: !include:tokenizer.yaml
\end{verbatim}

The last extension is the use of tuples. implicitly resolve any string starting with \inlinecode{(} and ending with \inlinecode{)} to a tuple.

\subsection{Other features}
SpeechBrain extends the PyTorch's data loading to help with speech data related challenges like handling variable length sequences and complicated data transformations. By leveraging PyTorch's \inlinecode{Dataset} and \inlinecode{DataLoader} classes, SpeechBrain enables on-the-fly data transformation (loading, augmentation, environment corruption, text processing, text encoding, feature extraction, etc) and can also be scaled to multiple parallel workers.

SpeechBrain also supports dynamic batching and multi GPU training, which are discussed in detail in Chapter 4. This enables large-scale speech recognition tasks, which is the core of the work done in this thesis.  